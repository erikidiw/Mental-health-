import pandas as pd
import joblib
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from category_encoders.target_encoder import TargetEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer

# ==========================
# ðŸ”§ Load Dataset
# ==========================
try:
    df = pd.read_csv("student_depression_dataset.csv")
except FileNotFoundError:
    print("Error: student_depression_dataset.csv tidak ditemukan. Pastikan file ada di direktori yang sama.")
    exit()

# ==========================
# ðŸ”§ Preprocessing Definitions
# ==========================

# 1. Cleaning function
def clean_data(df):
    df_copy = df.copy()
    # Cleaning quotes/spaces from categorical data (based on inspection)
    for col in ['Sleep Duration', 'Financial Stress']:
        if col in df_copy.columns:
            df_copy[col] = df_copy[col].astype(str).str.replace("'", "").str.strip()
            
    # Handle Financial Stress '?' as 0 for mapping
    df_copy['Financial Stress'] = df_copy['Financial Stress'].replace('?', '0')
    
    return df_copy

# 2. Define Features and Target
df_cleaned = clean_data(df)

# Drop unused/unnecessary columns
df_cleaned = df_cleaned.drop(columns=['id', 'Work Pressure', 'Job Satisfaction'], errors='ignore')

X = df_cleaned.drop(columns=['Depression'])
y = df_cleaned['Depression']

# 3. Define the Column Transformer (Preprocessing Steps)
# Ordinal Mapping (dikonversi ke numerik sebelum di-scale)
ordinal_mapping = [
    ('Sleep Duration', {'Less than 5 hours': 1, '5-6 hours': 2, '7-8 hours': 3, 'More than 8 hours': 4, 'Others': 0}),
    ('Financial Stress', {'1.0': 1, '2.0': 2, '3.0': 3, '4.0': 4, '5.0': 5, '0': 0}),
    ('Have you ever had suicidal thoughts ?', {'No': 0, 'Yes': 1}),
    ('Family History of Mental Illness', {'No': 0, 'Yes': 1}),
]

# Fungsi custom untuk menangani mapping Ordinal (untuk digunakan dalam Pipeline)
class CustomOrdinalMapper:
    def __init__(self, mappings):
        self.mappings = {col: map_dict for col, map_dict in mappings}
        self.cols = [col for col, _ in mappings]
        
    def fit(self, X, y=None):
        return self
        
    def transform(self, X):
        X_copy = X.copy()
        for col, mapping in self.mappings.items():
            if col in X_copy.columns:
                # Map, fillna 0, dan pastikan float
                X_copy[col] = X_copy[col].map(mapping).fillna(0).astype(float)
        return X_copy[self.cols]

# Jenis kolom
numerical_cols = ['Age', 'CGPA', 'Work/Study Hours', 'Academic Pressure', 'Study Satisfaction']
label_cols = ['Gender', 'Dietary Habits', 'Degree', 'Social Weakness']
target_cols = ['City', 'Profession']
ordinal_cols_names = [col for col, _ in ordinal_mapping]


# Membuat Preprocessor menggunakan ColumnTransformer dan Pipeline
preprocessor = ColumnTransformer(
    transformers=[
        ('ord', CustomOrdinalMapper(ordinal_mapping), ordinal_cols_names),
        ('label', LabelEncoder(), label_cols), # Label Encoder harus di luar CT jika ingin menyimpan class_
        ('target', TargetEncoder(min_samples_leaf=20, smoothing=10), target_cols),
        ('num', 'passthrough', numerical_cols)
    ],
    remainder='passthrough',
    verbose_feature_names_out=False
)

# NOTE: TargetEncoder membutuhkan target (y) untuk di-fit, jadi dia harus menjadi bagian dari Pipeline utama.
# LabelEncoder tidak bisa dipakai di ColumnTransformer karena menghasilkan 1 kolom saja.
# Solusi: Gunakan ColumnTransformer untuk Ordinal, Target, dan Numerik, lalu terapkan LabelEncoder secara manual di Pipeline.

# ==========================
# ðŸš€ Final Pipeline Setup
# ==========================

# Langkah 1: Preprocessing Custom (handling Ordinal and Target)
# TargetEncoder, LabelEncoder harus di-handle secara custom atau dalam skrip training terpisah.
# Karena kompleksitas integrasi encoder stateful ke CT, kita akan menggunakan pendekatan yang lebih sederhana:
# 1. Transform Ordinal/Target/Label secara manual pada X sebelum masuk ke Pipeline.
# 2. Pipeline hanya berisi StandardScaler dan Model.

# --- Replikasi Preprocessing dataset2.py untuk Training ---
# Langkah ini harus dijalankan untuk mendapatkan X_processed yang bersih untuk model.fit
X_processed = X.copy()

# Ordinal Mapping
custom_mapper = CustomOrdinalMapper(ordinal_mapping)
X_processed[ordinal_cols_names] = custom_mapper.fit_transform(X_processed)

# Label Encoding
le_encoders = {}
for col in label_cols:
    le = LabelEncoder()
    X_processed[col] = le.fit_transform(X_processed[col])
    le_encoders[col] = le

# Target Encoding
te = TargetEncoder(min_samples_leaf=20, smoothing=10)
X_processed[target_cols] = te.fit_transform(X_processed[target_cols], y)

# Final Pipeline (hanya Scaling dan Model)
pipeline = Pipeline(steps=[
    ('scaler', StandardScaler()),
    ('classifier', RandomForestClassifier(n_estimators=200, random_state=42))
])

# Fit Pipeline
pipeline.fit(X_processed, y)

# ==========================
# ðŸ’¾ Save Pipeline and Encoders
# ==========================

# Simpan objek yang diperlukan untuk prediksi
artifacts = {
    'pipeline': pipeline,
    'label_encoders': le_encoders,
    'target_encoder': te,
    'ordinal_mapper': custom_mapper,
    'feature_cols': X.columns.tolist(),
}

joblib.dump(artifacts, 'pipeline_artifacts.pkl')

print("\n-------------------------------------------------")
print("âœ… File 'pipeline_artifacts.pkl' berhasil dibuat.")
print("   File ini berisi model, scaler, dan semua encoder.")
print("-------------------------------------------------")
